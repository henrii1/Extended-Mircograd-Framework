{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q tiktoken"
      ],
      "metadata": {
        "id": "Yx9DR3KGJSGp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54bc2220-ee1c-420c-fd1c-c9fa5bc7193e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/1.1 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FsKO0JhTjR7O"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from dataclasses import dataclass\n",
        "import inspect    #helps with inspecting properties within an object\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# device agnostic code\n",
        "device = \"cpu\"\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "elif hasattr(torch.backends, \"mps\"):\n",
        "    device = \"mps\"\n",
        "\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "REAruE2zHAN4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd40ced6-ef2c-4af4-e680-437b42d4b27b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalSelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        # key, query, value projections for all heads, but in a batch\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
        "        # output projection\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        # regularization\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "        # not really a 'bias', more of a mask, but following the OpenAI/HF naming though\n",
        "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "                                     .view(1, 1, config.block_size, config.block_size))\n",
        "        self.c_proj.NANOGPT_WEIGHT_INIT = 1.0\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        # nh is \"number of heads\", hs is \"head size\", and C (number of channels) = nh * hs\n",
        "        # e.g. in GPT-2 (124M), n_head=12, hs=64, so nh*hs=C=768 channels in the Transformer\n",
        "        qkv = self.c_attn(x)\n",
        "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "\n",
        "        # # Normal attention (materializes the large (T,T) matrix for all the queries and keys)\n",
        "        # att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "        # att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
        "        # att = F.softmax(att, dim=-1)\n",
        "        # y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
        "\n",
        "        \"\"\"Switching to Flash Attention\"\"\"\n",
        "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
        "        # continuation same for causalattention\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "        # output projection\n",
        "        y = self.c_proj(y)\n",
        "        return y\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
        "        self.gelu    = nn.GELU(approximate='tanh')\n",
        "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd)\n",
        "        self.NANOGPT_WEIGHT_INIT = 1.0     #associated with the projection\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x\n",
        "\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int = 1024 # max sequence length\n",
        "    vocab_size: int = 50257 # number of tokens: 50,000 BPE merges + 256 bytes tokens + 1 <|endoftext|> token\n",
        "    n_layer: int = 12 # number of layers\n",
        "    n_head: int = 12 # number of heads\n",
        "    n_embd: int = 768 # embedding dimension\n",
        "\n",
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f = nn.LayerNorm(config.n_embd),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "\n",
        "        self.transformer.wte.weight = self.lm_head.weight  #weight sharing scheme linear.weight.shape(vocab_size, n_embd)\n",
        "\n",
        "        # apply this at initialization. we could have set this while defining each layer\n",
        "        self.apply(self._init_weights) #applies a function to an nn.Module object\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "          sd = 0.02\n",
        "          if hasattr(module, \"NANOGPT_WEIGHT_INIT\"):\n",
        "            sd *= (2 * self.config.n_layer)**-0.5\n",
        "          torch.nn.init.normal_(module.weight, mean=0.0, std=sd)\n",
        "          if module.bias is not None:\n",
        "            torch.nn.init.zeros_(module.bias)         #initializing bias. there is 'torch.nn.init.ones_'\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "          torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)   #intializing weights\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "      #input shape is B, T\n",
        "      B, T = idx.size()\n",
        "      assert T <= self.config.block_size, f\"cannot forward sequence of length {T} because the block size is only {self.config.block_size}\"\n",
        "      #position and token embedding\n",
        "      pos = torch.arange(0, T, dtype=torch.long, device=idx.device) #input would already have been sent to device\n",
        "      pos_emb = self.transformer.wpe(pos) # (T, n_embd)  #module dict can be accessed using '.'\n",
        "      tok_emb = self.transformer.wte(idx) # (B, T, n_embd)\n",
        "      x = tok_emb + pos_emb\n",
        "      #blocks\n",
        "      for block in self.transformer.h:\n",
        "        x = block(x)\n",
        "      #LayerNorm\n",
        "      x = self.transformer.ln_f(x)\n",
        "\n",
        "      logits = self.lm_head(x)  #B, T, vocab_size\n",
        "\n",
        "      loss = None\n",
        "      if targets is not None:\n",
        "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))  # logits should be 2d, targets should be 1d  (B*T, vocab_size, B*T)\n",
        "      return logits, loss\n",
        "\n",
        "\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, model_type):\n",
        "        \"\"\"Loads pretrained GPT-2 model weights from huggingface\"\"\"\n",
        "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
        "        from transformers import GPT2LMHeadModel\n",
        "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
        "\n",
        "        # n_layer, n_head and n_embd are determined from model_type\n",
        "        config_args = {\n",
        "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
        "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
        "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
        "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
        "        }[model_type]\n",
        "        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
        "        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
        "        # create a from-scratch initialized minGPT model\n",
        "        config = GPTConfig(**config_args)\n",
        "        model = GPT(config)\n",
        "        sd = model.state_dict()\n",
        "        sd_keys = sd.keys()\n",
        "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
        "\n",
        "        # init a huggingface/transformers model\n",
        "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
        "        sd_hf = model_hf.state_dict()\n",
        "\n",
        "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
        "        sd_keys_hf = sd_hf.keys()\n",
        "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
        "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
        "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
        "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
        "        # this means that we have to transpose these weights when we import them\n",
        "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
        "        for k in sd_keys_hf:\n",
        "            if any(k.endswith(w) for w in transposed):\n",
        "                # special treatment for the Conv1D weights we need to transpose\n",
        "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k].t())\n",
        "            else:\n",
        "                # vanilla copy over the other parameters\n",
        "                assert sd_hf[k].shape == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k])\n",
        "\n",
        "        return model\n",
        "\n",
        "    #configure optimizer\n",
        "    def configure_optimizers(self, weight_decay, learning_rate, device):\n",
        "        # start with all of the candidate parameters (that require grad)\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
        "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
        "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
        "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
        "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
        "        optim_groups = [\n",
        "            {'params': decay_params, 'weight_decay': weight_decay},\n",
        "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
        "        ]\n",
        "        num_decay_params = sum(p.numel() for p in decay_params)\n",
        "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
        "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
        "        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
        "        # Create AdamW optimizer and use the fused version if it is available\n",
        "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
        "        use_fused = fused_available and 'cuda' in device\n",
        "        print(f\"using fused AdamW: {use_fused}\")\n",
        "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=(0.9, 0.95), eps=1e-8, fused=use_fused)\n",
        "        return optimizer"
      ],
      "metadata": {
        "id": "KLa4u47AFIZZ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "import time\n",
        "# get a static batch of data from shakespare.txt\n",
        "# enc = tiktoken.get_encoding('gpt2')\n",
        "# with open('shakespare.txt', 'r', encoding='utf-8') as f:\n",
        "#   text = f.read()\n",
        "\n",
        "# text = [:1000]\n",
        "# tokens = enc.encode(text)\n",
        "# B, T = 4, 32\n",
        "# buf = torch.tensor(tokens[:B*T+1])\n",
        "# buf = buf.to(device)\n",
        "# x = buf[:-1].view(B, T)\n",
        "# y = buf[1:].view(B,T)\n",
        "\n",
        "\"\"\"creating an iterative dataloader class\"\"\"\n",
        "class DataLoaderLite:\n",
        "    def __init__(self, B, T):\n",
        "        self.B = B\n",
        "        self.T = T\n",
        "\n",
        "        # at init load tokens from disk and store them in memory\n",
        "        with open('shakespare.txt', 'r') as f:\n",
        "            text = f.read()\n",
        "        enc = tiktoken.get_encoding('gpt2')\n",
        "        tokens = enc.encode(text)\n",
        "        self.tokens = torch.tensor(tokens)\n",
        "        print(f\"loaded {len(self.tokens)} tokens\")\n",
        "        print(f\"1 epoch = {len(self.tokens) // (B * T)} batches\")\n",
        "\n",
        "        # state\n",
        "        self.current_position = 0\n",
        "\n",
        "    def next_batch(self):\n",
        "        B, T = self.B, self.T\n",
        "        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n",
        "        x = (buf[:-1]).view(B, T) # inputs\n",
        "        y = (buf[1:]).view(B, T) # targets\n",
        "        # advance the position in the tensor\n",
        "        # self.curr_pos += (B*T)+(B*T*torch.randint(-3, 3, (1,)).item()) #randomly assigns batches. add condition for negative positions.\n",
        "        self.current_position += B * T\n",
        "        # if loading the next batch would be out of bounds, reset\n",
        "        if self.current_position + (B * T + 1) > len(self.tokens):\n",
        "            self.current_position = 0\n",
        "        return x, y\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tqNa-gK6JqMT"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "torch.manual_seed(42)\n",
        "if torch.cuda.is_available():\n",
        "  torch.cuda.manual_seed(42)\n",
        "#TF32 matmuls\n",
        "torch.set_float32_matmul_precision('high')  # TF32 computation, 'medium' is bf16\n",
        "\n",
        "# #without gradient accumulation\n",
        "# train_data = DataLoaderLite(16, 1024)\n",
        "\n",
        "#with gradient accumulation\n",
        "total_batch_size = 524288    #A nice number, close to the OpenAI batch size\n",
        "B = 4\n",
        "T = 1024\n",
        "assert total_batch_size % (B*T) == 0  #confirm divisibility for grad accumulation\n",
        "grad_accum_steps = total_batch_size // (B*T)\n",
        "print(f\"desired batch size according to OpenAI: {total_batch_size}, grad accumulation steps: {grad_accum_steps}\")\n",
        "train_loader = DataLoaderLite(B, T)\n",
        "\n",
        "\n",
        "# model = GPT(GPTConfig())\n",
        "model = GPT(GPTConfig(vocab_size=50304))  #increasing the trainable vocab_size, using nice number for even computation\n",
        "model.to(device)\n",
        "\n",
        "#add torch.compile\n",
        "model = torch.compile(model)\n",
        "\n",
        "#adding a learning rate scheduler (OpenAI uses a cosine lr schedule)\n",
        "max_lr = 6e-4\n",
        "min_lr = max_lr * 0.1\n",
        "warmup_steps = 10\n",
        "max_steps = 50\n",
        "\n",
        "def get_lr(it):\n",
        "  if it < warmup_steps:\n",
        "    return max_lr * (it+1) /warmup_steps\n",
        "  if it > max_steps:\n",
        "    return min_lr\n",
        "\n",
        "  decay_ratio = (it - warmup_steps) / (max_steps - warmup_steps)\n",
        "  assert 0 <= decay_ratio <= 1\n",
        "  coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))  # start at one and decay to zero\n",
        "\n",
        "  return min_lr + coeff * (max_lr - min_lr)\n",
        "\n",
        "\n",
        "# optimizer\n",
        "# optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
        "# optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, betas=(0.9, 0.95), eps=1e-8)    #OpenAI settings\n",
        "optimizer = model.configure_optimizers(weight_decay=0.1, learning_rate=6e-4, device=device)\n",
        "\n",
        "#training loop\n",
        "for step in range(max_steps):\n",
        "  t0 = time.time()\n",
        "  optimizer.zero_grad()  #never forget to zero the gradient after each step\n",
        "\n",
        "  # #without gradient accumulation\n",
        "  # x, y = train_data.next_batch()\n",
        "  # x, y = x.to(device), y.to(device)\n",
        "  # #cast loss and logits operation to bfloat16\n",
        "  # with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
        "  #     logits, loss = model(x, y)\n",
        "  # loss.backward()\n",
        "\n",
        "  #with gradient accumulation\n",
        "  loss_accum = 0.0\n",
        "  for _ in range(grad_accum_steps):\n",
        "    x, y = train_loader.next_batch()\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    #cast loss and logits operation to bfloat16\n",
        "\n",
        "    \"\"\"Bfloat16 is not suported by T4 GPU\"\"\"\n",
        "    # with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
        "    #   logits, loss = model(x, y)  #a lot aof changes can be made\n",
        "\n",
        "    logits, loss = model(x, y)\n",
        "    loss = loss / grad_accum_steps  #this is relevant otherwise the actual value would equal our value divided by accum_no. check with small example\n",
        "    loss_accum += loss.detach()  #detach instead of item because detach preserves value as tensor (its possible to += a tensor to a scalar value)\n",
        "    loss.backward()\n",
        "\n",
        "  # clip gradient to not exceed 1\n",
        "  norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "  #include lr from scheduler\n",
        "  lr = get_lr(step)\n",
        "  for param_group in optimizer.param_groups:  #optimizer object has an attribute called param_groups\n",
        "    param_group['lr'] = lr\n",
        "\n",
        "  optimizer.step()\n",
        "  torch.cuda.synchronize()  #synchronize the cpu and gpu times.\n",
        "  t1 = time.time()\n",
        "  dt = t1 - t0\n",
        "\n",
        "  # #without grad accum\n",
        "  # tokens_processed = train_loader.B * train_loader.T\n",
        "\n",
        "  #with grad accum\n",
        "  tokens_processed = grad_accum_steps * train_loader.B * train_loader.T\n",
        "\n",
        "  tokens_per_sec = tokens_processed / dt\n",
        "\n",
        "  print(f\"step{step:4d} | loss: {loss_accum.item():.6f} | {lr:.4e} | norm: {norm} | dt:{dt*1000:.2f}ms | tok/sec: {tokens_per_sec}\")"
      ],
      "metadata": {
        "id": "fBibisRLHtgp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e760c57f-fb48-444f-9694-2c7cefb07a3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "desired batch size according to OpenAI: 524288, grad accumulation steps: 128\n",
            "loaded 338025 tokens\n",
            "1 epoch = 82 batches\n",
            "num decayed parameter tensors: 50, with 124,354,560 parameters\n",
            "num non-decayed parameter tensors: 98, with 121,344 parameters\n",
            "using fused AdamW: True\n",
            "step   0 | loss: 10.977739 | 6.0000e-05 | norm: 7.347934722900391 | dt:178321.68ms | tok/sec: 2940.1248496601306\n",
            "step   1 | loss: 9.790614 | 1.2000e-04 | norm: 4.912006378173828 | dt:134626.98ms | tok/sec: 3894.375473821962\n",
            "step   2 | loss: 9.363747 | 1.8000e-04 | norm: 8.77891731262207 | dt:134536.00ms | tok/sec: 3897.0090269653087\n",
            "step   3 | loss: 9.325029 | 2.4000e-04 | norm: 4.76426362991333 | dt:134440.58ms | tok/sec: 3899.774803690933\n",
            "step   4 | loss: 8.904864 | 3.0000e-04 | norm: 3.6641361713409424 | dt:134430.49ms | tok/sec: 3900.067479236453\n",
            "step   5 | loss: 8.545564 | 3.6000e-04 | norm: 2.050356388092041 | dt:134138.61ms | tok/sec: 3908.5539357986936\n",
            "step   6 | loss: 8.289697 | 4.2000e-04 | norm: 2.533982992172241 | dt:134032.14ms | tok/sec: 3911.6588001597183\n",
            "step   7 | loss: 7.976324 | 4.8000e-04 | norm: 2.246410846710205 | dt:134118.79ms | tok/sec: 3909.1315598269202\n",
            "step   8 | loss: 7.622236 | 5.4000e-04 | norm: 2.2899692058563232 | dt:134264.25ms | tok/sec: 3904.896395567653\n",
            "step   9 | loss: 7.258352 | 6.0000e-04 | norm: 1.7254353761672974 | dt:134312.57ms | tok/sec: 3903.4917778599183\n",
            "step  10 | loss: 6.912235 | 6.0000e-04 | norm: 1.22917640209198 | dt:134312.41ms | tok/sec: 3903.4962817789387\n",
            "step  11 | loss: 6.638340 | 5.9917e-04 | norm: 1.0499075651168823 | dt:134160.49ms | tok/sec: 3907.916422760496\n",
            "step  12 | loss: 6.424485 | 5.9668e-04 | norm: 1.2096754312515259 | dt:134491.90ms | tok/sec: 3898.2866809735997\n",
            "step  13 | loss: 6.373016 | 5.9254e-04 | norm: 2.0417113304138184 | dt:134162.40ms | tok/sec: 3907.8609205833836\n",
            "step  14 | loss: 6.249706 | 5.8679e-04 | norm: 1.6079576015472412 | dt:134400.26ms | tok/sec: 3900.9447799450118\n",
            "step  15 | loss: 6.154429 | 5.7945e-04 | norm: 0.5601836442947388 | dt:134214.79ms | tok/sec: 3906.3356446297207\n",
            "step  16 | loss: 6.133570 | 5.7057e-04 | norm: 1.6611875295639038 | dt:134468.86ms | tok/sec: 3898.954604254814\n",
            "step  17 | loss: 6.082034 | 5.6021e-04 | norm: 1.0149567127227783 | dt:134498.61ms | tok/sec: 3898.0923429768222\n",
            "step  18 | loss: 6.056663 | 5.4843e-04 | norm: 0.6673746705055237 | dt:134377.96ms | tok/sec: 3901.592272738876\n",
            "step  19 | loss: 6.012069 | 5.3531e-04 | norm: 0.7618101239204407 | dt:134516.98ms | tok/sec: 3897.560128972109\n",
            "step  20 | loss: 6.009098 | 5.2092e-04 | norm: 0.5115343332290649 | dt:134495.09ms | tok/sec: 3898.19430184292\n",
            "step  21 | loss: 5.946898 | 5.0535e-04 | norm: 0.4937093257904053 | dt:134585.94ms | tok/sec: 3895.5629149379442\n",
            "step  22 | loss: 5.939293 | 4.8870e-04 | norm: 0.647035539150238 | dt:134642.78ms | tok/sec: 3893.9184439501228\n",
            "step  23 | loss: 5.913909 | 4.7107e-04 | norm: 0.8964425921440125 | dt:134519.42ms | tok/sec: 3897.4892052556775\n",
            "step  24 | loss: 5.881648 | 4.5258e-04 | norm: 0.39047756791114807 | dt:134538.78ms | tok/sec: 3896.928323926241\n",
            "step  25 | loss: 5.875180 | 4.3332e-04 | norm: 0.8716546297073364 | dt:134611.99ms | tok/sec: 3894.8090807220083\n",
            "step  26 | loss: 5.855186 | 4.1343e-04 | norm: 0.7233177423477173 | dt:134497.73ms | tok/sec: 3898.117868470995\n",
            "step  27 | loss: 5.848735 | 3.9303e-04 | norm: 0.5331918597221375 | dt:134578.58ms | tok/sec: 3895.7761047061863\n",
            "step  28 | loss: 5.809733 | 3.7224e-04 | norm: 0.8133103847503662 | dt:134444.21ms | tok/sec: 3899.6697262092475\n",
            "step  29 | loss: 5.822627 | 3.5118e-04 | norm: 0.6959964036941528 | dt:134574.58ms | tok/sec: 3895.8916569480198\n",
            "step  30 | loss: 5.784006 | 3.3000e-04 | norm: 0.5150112509727478 | dt:134576.75ms | tok/sec: 3895.8289174833485\n",
            "step  31 | loss: 5.774246 | 3.0882e-04 | norm: 0.7873464822769165 | dt:134526.04ms | tok/sec: 3897.297509302138\n",
            "step  32 | loss: 5.754768 | 2.8776e-04 | norm: 0.46119117736816406 | dt:134568.54ms | tok/sec: 3896.0665788959536\n",
            "step  33 | loss: 5.738067 | 2.6697e-04 | norm: 0.6083870530128479 | dt:134661.39ms | tok/sec: 3893.3801724358605\n",
            "step  34 | loss: 5.724304 | 2.4657e-04 | norm: 0.5439442992210388 | dt:134427.95ms | tok/sec: 3900.141305204542\n",
            "step  35 | loss: 5.694544 | 2.2668e-04 | norm: 0.4148474633693695 | dt:134548.41ms | tok/sec: 3896.6494041643073\n",
            "step  36 | loss: 5.708008 | 2.0742e-04 | norm: 0.65260249376297 | dt:134575.13ms | tok/sec: 3895.875989147625\n",
            "step  37 | loss: 5.656994 | 1.8893e-04 | norm: 0.4230342507362366 | dt:134575.81ms | tok/sec: 3895.856152650722\n",
            "step  38 | loss: 5.670061 | 1.7130e-04 | norm: 0.399044007062912 | dt:134469.02ms | tok/sec: 3898.9500002024806\n",
            "step  39 | loss: 5.642426 | 1.5465e-04 | norm: 0.5061500668525696 | dt:134495.54ms | tok/sec: 3898.181365797087\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluation code\n",
        "model.eval()\n",
        "num_return_sequences=5\n",
        "max_length = 30\n",
        "\n",
        "enc = tiktoken.get_encoding('gpt2')\n",
        "tokens = enc.encode(\"Hello, I'm a language model\") # (8,)\n",
        "tokens = torch.tensor(tokens, dtype=torch.long)\n",
        "tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1)  #(5, 8)  B, T\n",
        "x = tokens.to(device)\n",
        "\n",
        "#generate\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "while x.size(1) < max_length:\n",
        "  # forward pass\n",
        "  with torch.no_grad():\n",
        "    logits = model(x)  #B, T, vocab_size\n",
        "    logits = logits[:, -1, :]    #B, vocab_size\n",
        "\n",
        "    probs = F.softmax(logits, dim=1)  #we always do softmax on the last dim\n",
        "\n",
        "    # top-K  changes from (5, vocab_size) to (5, 50)\n",
        "    topk_probs, top_indices = torch.topk(probs, 50, dim=-1)\n",
        "\n",
        "    ix = torch.multinomial(topk_probs, 1)  #B, 1\n",
        "    #because our topk_prob is a sample, we'll have to retrieve its indicies\n",
        "    xcol = torch.gather(top_indices, -1, ix)   #B, 1    index ix from top_indices and gather it in the column\n",
        "    #append to sequence\n",
        "    x = torch.cat((x, xcol), dim=1)\n",
        "\n",
        "\n",
        "# print generated text\n",
        "for i in range(num_return_sequences):\n",
        "  tokens = x[i, :max_length].tolist()    #can't use decode on tensor\n",
        "  decoded = enc.decode(tokens)\n",
        "  print(\"Gen\", decoded)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1ibBfS8-HOVw",
        "outputId": "08b0dc1d-7974-492e-b113-6f9e4fe94cf1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading weights from pretrained gpt: gpt2\n",
            "Gen Hello, I'm a language modeler. In fact I use that name from C# to describe our development and development workflow. We have many different tools for this.\n",
            "\n",
            "The best thing about these tools is that they are designed from a pure, easy to use and easy to understand point of view.\n",
            "\n",
            "A lot of developers feel that having a full-stack application comes down to \"let's be great and focus on our own business\" rather than \"let's focus on all the\n",
            "Gen Hello, I'm a language model. Language models are not that complicated unless you know to change the behavior of some of your functions, of the variable name, of the variable body and I mean all of them. So I said, \"Let's look at our model for variable named \"Foo\". Well I don't have a variable named Foo, just I have a class. The constructor is created with this 'Foo to Foo' operator:\n",
            "\n",
            "function foo(expression) { var\n",
            "Gen Hello, I'm a language model. I make the syntax based on it. For example, it can be used to define the syntax of a language based on a syntax tree.\n",
            "\n",
            "But I want to make sure that if someone uses the type of A the program will be run as usual. I'm trying to be as fair as possible to the language as far as possible.\n",
            "\n",
            "I would argue that if there is a problem with an C library, there are problems with implementing it in\n",
            "Gen Hello, I'm a language modeler - I like to do a lot of abstraction and I've also seen the ability to generate generic programming languages like JavaScript and Scala or even OCaml.\n",
            "\n",
            "HaskellJS\n",
            "\n",
            "What is Haskell for you?\n",
            "\n",
            "I'm a Haskell programmer, so I'm aware Haskell is not for me, but I love that there are so many wonderful tools that are available in the Haskell ecosystem.\n",
            "\n",
            "Haskell is not what most of us think it\n",
            "Gen Hello, I'm a language model that has a language model in the domain of computer biology. If you study computer systems in your undergraduate degree or at a top-security security firm, that provides you with an important experience to understand the implications of algorithms for the security of your systems. In my language model, these algorithms are the type of computers that actually use your system, the human, to create things, modify them, and perform all their operations. We have that advantage over people who don't\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hx5f-niV6jQ",
        "outputId": "0f416e32-952a-475c-eb49-b9c3ae037659"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(5.)\n"
          ]
        }
      ]
    }
  ]
}