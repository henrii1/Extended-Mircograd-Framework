{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViJJ8sqxXi5u"
      },
      "source": [
        "##Free Base Data Format\n",
        "\n",
        "### Entities\n",
        "```csv\n",
        "entity_id,name,type\n",
        "/m/01g3,Barack Obama,Person\n",
        "/m/02hrh,Google,Organization\n",
        "/m/07s9,The Great Wall of China,Location\n",
        "...\n",
        "\n",
        "```\n",
        "### Relations\n",
        "```csv\n",
        "relation_id,entity_id1,entity_id2\n",
        "/people/person/spouse,/m/01g3,/m/0c1v\n",
        "/organization/organization/founders,/m/02hrh,/m/0c1v\n",
        "/location/location/contains,/m/07s9,/m/0c1v\n",
        "...\n",
        "\n",
        "```\n",
        "### Node Properties (Features)\n",
        "```csv\n",
        "entity_id,property,value\n",
        "/m/01g3,/people/person/birth_date,1961-08-04\n",
        "/m/01g3,/people/person/education,/m/0c1v\n",
        "/m/02hrh,/organization/organization/founding_date,1998-09-04\n",
        "...\n",
        "\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jx4fs9AEihiE"
      },
      "source": [
        "Indexing pathquery datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PC4eyyLMXYO2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.sparse import lil_matrix, csr_matrix  #sparse matrix for adj_mat\n",
        "\n",
        "class Vocab(object):\n",
        "  def __init__(self):\n",
        "    self.word2id = {}\n",
        "    self.id2word = []\n",
        "\n",
        "  def add(self, word):\n",
        "    if word not in self.word2id:\n",
        "      self.word2id[word] = len(self.id2word)\n",
        "      self.id2word.append(word)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.id2word)\n",
        "\n",
        "  #get the id using Vocab()['word'], like call but we index [] with getitem and we call functions\n",
        "  #with call ()\n",
        "  def __getitem__(self, word):\n",
        "    return self.word2id[word]\n",
        "\n",
        "\n",
        "\n",
        "  @classmethod\n",
        "  def load(cls, vocab_path):  #alternative method of initializing class\n",
        "    v = Vocab()\n",
        "    with open(vocab_path, 'r') as f:\n",
        "      for word in f:    #loop over lines for txt files\n",
        "        v.add(word.strip())  #striping spaces and newline and adding just the word\n",
        "    return v\n",
        "\n",
        "\n",
        "class Dataset(object):\n",
        "  def __init__(self, samples):\n",
        "    assert type(samples) == list or type(samples) == np.ndarray\n",
        "    self._samples = samples if type(samples) == np.ndarray else np.array(samples)\n",
        "\n",
        "  def __getitem__(self, item):\n",
        "    return self._samples[item]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self._samples)\n",
        "\n",
        "  def batch_iter(self, batchsize, rand_flg=True):\n",
        "    indices = np.random.permutation(len(self)) if rand_flg else np.arange(len(self))\n",
        "    for start in range(0, len(self), batchsize):\n",
        "      yield self[indices[start:start+batchsize]] #using yield is faster, more efficient and stateful. it loads only current batch to memory\n",
        "\n",
        "  @classmethod\n",
        "  def load(cls, data_path, ent_vocab, rel_vocab):\n",
        "    raise NotImplementedError\n",
        "\n",
        "\n",
        "class TripleDataset(Dataset):  #src, rel, dst\n",
        "  def __init__(self, samples):\n",
        "    super().__init__(samples=samples)\n",
        "\n",
        "  @classmethod\n",
        "  def load(cls, data_path, ent_vocab, rel_vocab):\n",
        "    samples = []\n",
        "    with open(data_path, \"r\") as f:\n",
        "      for line in f:\n",
        "        sub, rel, obj = line.strip().split(\"\\t\")\n",
        "        samples.append((ent_vocab[sub], rel_vocab[rel], ent_vocab[obj]))\n",
        "    return cls(samples)\n",
        "\n",
        "\n",
        "class TensorTypeGraph(object):    #fills up an edge list based on sample list\n",
        "  def __init__(self, triple_dat, n_ent, n_rel):\n",
        "    self.rel2mat = [lil_matrix((n_ent, n_ent)) for _ in range(n_rel)] #whether any two entity has any relationship\n",
        "    for triple in triple_dat.batch_iter(1, rand_flag=False): #batch_iter method is iterable\n",
        "      sub, rel, obj = triple[0] #sample from triplet dataset\n",
        "      self.rel2mat[rel][sub, obj] = 1.0   #fill up relational matrix\n",
        "\n",
        "  def search_obj_id(self, sub, rel):\n",
        "    return np.where(self.rel2mat[rel][sub].todense() == 1)[1]  #column index\n",
        "\n",
        "  def search_sub_id(self, obj, rel):\n",
        "    return np.where(self.rel2mat[rel][:, obj].todense() == 1)[0]  #row index (because of nonsymmetry)\n",
        "\n",
        "  @classmethod\n",
        "  def load_from_raw(cls, data_path, ent_v, rel_v):\n",
        "    triples = TripletDataset.load(data_path, ent_v, rel_v)\n",
        "    return cls(triples, len(ent_v), len(rel_v))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SuVMNE7B72f3"
      },
      "source": [
        "####Load dataset of form + pytorch training\n",
        "```json\n",
        "data = {\n",
        "        'head': ['A', 'B', 'C', 'A', 'C'],\n",
        "        'relation': ['r1', 'r2', 'r3', 'r1', 'r2'],\n",
        "        'tail': ['B', 'C', 'A', 'C', 'B']\n",
        "    }\n",
        "\n",
        "  ```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nSsyWiLk71kw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQaKPfHl71v5"
      },
      "outputs": [],
      "source": [
        "#load data\n",
        "def load_data(file_path):\n",
        "  data = pd.read_csv(file_path, sep=\"\\t\", header=None)\n",
        "  data.columns = ['head', 'relation', 'tail']\n",
        "  return data\n",
        "\n",
        "def create_mappings(data):\n",
        "  entities = set(data[\"head\"]).union(set(data[\"tail\"]))\n",
        "  relations = set(data[\"relation\"])\n",
        "\n",
        "  entity_to_id = {entity: i for i, entity in enumerate(entities)}\n",
        "  relation_to_id = {relation: i for i, relation in enumerate(relations)}\n",
        "\n",
        "  return entity_to_id, relation_to_id\n",
        "\n",
        "def encode_triplets(data, entity_to_id, relation_to_id):\n",
        "  head_ids = data[\"head\"].map(entity_to_id).values   #a pd method\n",
        "  relation_ids = data[\"relation\"].map(relation_to_id).values\n",
        "  tail_ids = data[\"tail\"].map(entity_to_id).values   #1d np array of idx\n",
        "\n",
        "  return head_ids, relation_ids, tail_ids\n",
        "\n",
        "\n",
        "data = load_data(\"FB15k.txt\")\n",
        "entity_to_id, relation_to_id = create_mappings(data)\n",
        "head_ids, relation_ids, tail_ids = encode_triplets(data, entity_to_id, relation_to_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6H_XinQiGe2a"
      },
      "outputs": [],
      "source": [
        "#TransE model\n",
        "class TransE(nn.Module):\n",
        "  def __init__(self, num_entities, num_relations, embedding_dim, margin=1.0):\n",
        "    super().__init__()\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.margin = margin\n",
        "\n",
        "    self.entity_embeddings = nn.Embedding(num_entities, embedding_dim)\n",
        "    self.relation_embeddings = nn.Embedding(num_relations, embedding_dim)\n",
        "\n",
        "    #initialize embeddings\n",
        "    nn.init.xavier_uniform_(self.entity_embeddings.weight.data)  #weight is fine\n",
        "    nn.init.xavier_uniform_(self.relation_embeddings.weight.data)\n",
        "\n",
        "  def forward(self, heads, relations, tails, negative_heads, negative_tails):\n",
        "    head_emb = self.entity_embeddings(heads)\n",
        "    relation_emb = self.relation_embeddings(relations)\n",
        "    tail_emb = self.entity_embeddings(tails)\n",
        "    negative_head_emb = self.entity_embeddings(negative_heads)\n",
        "    negative_tail_emb = self.entity_embeddings(negative_tails)\n",
        "\n",
        "    pos_distance = torch.linalg.norm(head_emb + relation_emb - tail_emb, ord=1, dim=1) #reduce operation\n",
        "    neg_distance = torch.linalg.norm(negative_head_emb + relation_emb - negative_tail_emb, ord=1, dim=1)  #TransE\n",
        "\n",
        "    return pos_distance, neg_distance\n",
        "\n",
        "  def loss(self, pos_distance, neg_distance):\n",
        "    # pos_distance should be less than neg_distance by a margin. loss will be high until this is obtained\n",
        "    return torch.mean(torch.relu(self.margin+pos_distance - neg_distance))\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AzlpX4EzGe4o",
        "outputId": "ebf42b61-dd86-4772-84b1-6846bc2ffd5b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([3., 7.])"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#generate negative samples\n",
        "def generate_negative_samples(heads, tails, relation):\n",
        "  num_entities = max(max(heads), max(tails)) + 1\n",
        "  neg_dict = defaultdict(list)\n",
        "  head_idx, tail_idx, relation_idx = list(heads), list(tails), list(relation)\n",
        "  rel_idx = 0\n",
        "  while True:\n",
        "    neg_head = random.choice(range(num_entities))\n",
        "    neg_tail = random.randint(0, num_entities-1)\n",
        "    if (neg_head, relation_idx[rel_idx], neg_tail) not in zip(head_idx, relation_idx, tail_idx):\n",
        "      neg_dict['head'].append(neg_head)\n",
        "      neg_dict['tail'].append(neg_tail)\n",
        "      rel_idx += 1\n",
        "    if len(neg_dict['head']) == len(head_idx):\n",
        "      break\n",
        "  neg_heads = np.array(neg_dict['head'])\n",
        "  neg_tails = np.array(neg_dict['tail'])\n",
        "\n",
        "  return neg_heads, neg_tails\n",
        "\n",
        "def train_model(model, data, entity_to_id, relation_to_id, epochs=100, batch_size=128, lr=0.001):\n",
        "  optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "  head_ids, relation_ids, tail_ids = encode_triplets(data, entity_to_id, relation_to_id)\n",
        "  num_entities = len(entity_to_id)\n",
        "  num_batches = len(head_ids) // batch_size\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    total_loss = 0.0\n",
        "    for batch in range(num_batches):\n",
        "      batch_start = batch * batch_size\n",
        "      batch_end = (batch + 1) * batch_size\n",
        "      heads = torch.tensor(head_ids[batch_start:batch_end])\n",
        "      relations = torch.tensor(relation_ids[batch_start:batch_end])\n",
        "      tails = torch.tensor(tail_ids[batch_start:batch_end])\n",
        "\n",
        "      negative_heads, negative_tails = generate_negative_samples(heads, tails, relations)\n",
        "      negative_heads = torch.tensor(negative_heads)\n",
        "      negative_tails = torch.tensor(negative_tails)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      pos_distance, neg_distance = model(heads, relations, tails, negative_heads, negative_tails)\n",
        "      loss = model.loss(pos_distance, neg_distance)\n",
        "\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      total_loss += loss.item()\n",
        "    avg_loss = total_loss / num_batches\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Average Loss: {avg_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPFBSNP4jEQT"
      },
      "source": [
        "### Random Graph Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9hobA9SbGe7u"
      },
      "outputs": [],
      "source": [
        "#erdos renyi random graph model\n",
        "import torch\n",
        "from torch_geometric.utils import erdos_renyi_graph\n",
        "import torch_geometric\n",
        "from torch_geometric.data import Data\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Erdos params\n",
        "num_nodes = 100\n",
        "edge_prob = 0.1\n",
        "\n",
        "#generated edge index\n",
        "edge_index = erdos_renyi_graph(num_nodes, edge_prob)\n",
        "\n",
        "#creating a PyG Data Object\n",
        "data = Data(edge_index=edge_index)\n",
        "\n",
        "def plot_graph(data):\n",
        "  G = nx.Graph()\n",
        "  G.add_edges_from(data.edge_index.t().tolist())\n",
        "  pos = nx.spring_layout(G, seed=1)\n",
        "  nx.draw(G, pos, with_labels=True, node_color=\"skyblue\", node_size=500,edge_color=\"gray\")\n",
        "  plt.show()\n",
        "\n",
        "plot_graph(data)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
