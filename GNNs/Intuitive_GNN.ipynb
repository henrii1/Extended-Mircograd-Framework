{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "!pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-1.13.1+cu116.html\n",
        "!pip install torch-sparse -f https://pytorch-geometric.com/whl/torch-1.13.1+cu116.html\n",
        "!pip install torch-geometric\n",
        "!pip install -q git+https://github.com/snap-stanford/deepsnap.git"
      ],
      "metadata": {
        "id": "IhyC_RlGcURE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57bf1417-1086-49fa-c286-cd3f4337e0dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.13.1+cu116.html\n",
            "Collecting torch-scatter\n",
            "  Downloading torch_scatter-2.1.2.tar.gz (108 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.0/108.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: torch-scatter\n",
            "  Building wheel for torch-scatter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-scatter: filename=torch_scatter-2.1.2-cp310-cp310-linux_x86_64.whl size=507268 sha256=c97fe41fd54240c4e29c99b4125286b0d83a1c8a77848ae2e8a11ceccbf227f1\n",
            "  Stored in directory: /root/.cache/pip/wheels/92/f1/2b/3b46d54b134259f58c8363568569053248040859b1a145b3ce\n",
            "Successfully built torch-scatter\n",
            "Installing collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.1.2\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.13.1+cu116.html\n",
            "Collecting torch-sparse\n",
            "  Downloading torch_sparse-0.6.18.tar.gz (209 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.0/210.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-sparse) (1.11.4)\n",
            "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy->torch-sparse) (1.25.2)\n",
            "Building wheels for collected packages: torch-sparse\n",
            "  Building wheel for torch-sparse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-sparse: filename=torch_sparse-0.6.18-cp310-cp310-linux_x86_64.whl size=1092566 sha256=9e744344ead437430156d2716244ce634d2dbd82cc690c959bab5087aa4f0a25\n",
            "  Stored in directory: /root/.cache/pip/wheels/c9/dd/0f/a6a16f9f3b0236733d257b4b4ea91b548b984a341ed3b8f38c\n",
            "Successfully built torch-sparse\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.18\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.5.3-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.66.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.11.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2023.6.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.4)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.9.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2024.7.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (3.5.0)\n",
            "Installing collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.5.3\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for deepsnap (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch_geometric\n",
        "torch_geometric.__version__"
      ],
      "metadata": {
        "id": "zqOMQ217cUNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch_scatter\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torch_geometric.nn as pyg_nn\n",
        "import torch_geometric.utils as pyg_utils\n",
        "\n",
        "from torch import Tensor\n",
        "from typing import Union, Tuple, Optional\n",
        "from torch_geometric.typing import (Adj, Size, NoneType,\n",
        "                                    OptPairTensor, OptTensor)\n",
        "\n",
        "from torch.nn import Linear, Parameter\n",
        "form torch_sparse import SparseTensor, set_diag\n",
        "from torch_scatter import scatter\n",
        "from torch_geometric.nn.conv import MessagePassing\n",
        "from torch_geometric.utils import remove_self_loops, add_self_loops, softmax"
      ],
      "metadata": {
        "id": "rR4ijZm2HKc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#defining the GNN stack\n",
        "class GNNStack(nn.Module):\n",
        "  def __init__(self, input_dim, hidden_dim, output_dim, args, emb=False):\n",
        "    super().__init__()\n",
        "    gnn_model = GAT if args.model_type.lower() == \"gat\" else GraphSage\n",
        "    self.model = nn.Modulelist([gnn_model(input_dim, hidden_dim)])\n",
        "    for layer in range(args.num_layers - 1):\n",
        "      self.model.append(gnn_model(hidden_dim * args.heads, hidden_dim))\n",
        "\n",
        "    #post-message-passing\n",
        "    self.after_mp = nn.Sequential(\n",
        "        nn.Linear(hidden_dim * args.heads, hidden_dim),\n",
        "        nn.dropout(args.dropout),\n",
        "        nn.Linear(hidden_dim, output_dim)\n",
        "    )\n",
        "\n",
        "    self.num_layers = args.num_layers\n",
        "    self.dropout = args.dropout\n",
        "    self.emb = emb\n",
        "\n",
        "  def forward(self, data):\n",
        "    x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x = self.model[i](x, edge_index)\n",
        "      x = F.relu(x)\n",
        "      x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "\n",
        "    x = self.post_mp(x)\n",
        "\n",
        "    if self.emb:\n",
        "      return x\n",
        "\n",
        "    return F.Log_softmax(x, dim=1)\n",
        "\n",
        "  def loss(self, pred, label):\n",
        "    return F.nll_loss(pred, label)\n",
        "\n"
      ],
      "metadata": {
        "id": "FqOc-PT1HKf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#implementing the GraphSage class. it inherits from messagepassing\n",
        "class GraphSage(MessagePassing):\n",
        "  def __init__(self, in_channels, out_channels, normalize=True, bias=False, **kwargs):\n",
        "    super().__init__()\n",
        "\n",
        "    self.in_channels = in_channels\n",
        "    self.out_channels = out_channels\n",
        "    self.normalize = normalize\n",
        "\n",
        "    self.lin_l = Linear(in_channels, out_channels, bias=bias)\n",
        "    self.lin_r = Linear(in_channels, out_channels, bias=bias)\n",
        "\n",
        "    self.reset_params()\n",
        "\n",
        "  def reset_params(self):\n",
        "    self.lin_l.reset_parameters()\n",
        "    self.lin_r.reset_parameters()\n",
        "\n",
        "  def forward(self, x, edge_index, size=None):\n",
        "    agg_message = self.propagate(edge_index, x=x, size=size)\n",
        "    out = self.lin_l(x) + self.lin_r(agg_message)\n",
        "    if self.normalize:\n",
        "      out = F.normalize(out)\n",
        "\n",
        "    return out\n",
        "\n",
        "  def message(self, x_j):         #x_j is the feature vector of the source node\n",
        "    return x_j                    # This is computed for each neighbor iteratively by refering to edge tensor\n",
        "\n",
        "  def aggregate(self, inputs, index, dim_size=None):\n",
        "    return scatter(inputs, index, dim=self.node_dim, dim_size=dim_size, reduce=\"mean\")\n"
      ],
      "metadata": {
        "id": "LH6nSmJaHKjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#implementing graph_sage layer from scratch\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from torch_scatter import scatter_add, scatter_softmax\n",
        "from torch_sparse import SparseTensor\n",
        "#considering the use of dense tensors for edge index\n",
        "class GraphSageD(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, normalize=False):\n",
        "    super().__init__()\n",
        "    #in_channels times two because of the cat operation on the feature dim at the end. in_channels = num_features\n",
        "    self.lin = nn.Linear(in_channels * 2, out_channels)   #out_channels should equal the expected embedding dimension after computation\n",
        "\n",
        "  def forward(self, x, edge_index):\n",
        "    num_nodes = x.size(0)\n",
        "    loop_index = torch.arange(0, num_nodes, dtype=torch.long, device=x.device)\n",
        "    loop_index = loop_index.unsqueeze(0).repeat(2, 1)  #repeat for source and target\n",
        "\n",
        "    #loop_index tries to add nodes own message during the computation\n",
        "    edge_index = torch.cat([edge_index, loop_index], dim=1)  #edge_index is of shape 2, num_edges\n",
        "\n",
        "    #compute messages\n",
        "    if isinstance(edge_index, SparseTensor):\n",
        "      source, target = edge_index.to_dense()       #we can use .values() too\n",
        "    else:\n",
        "      source, target = edge_index\n",
        "\n",
        "    x_j = x[target]         #indexing out the target features  (target, feature_dim)\n",
        "\n",
        "    #Aggregate messages\n",
        "    out = scatter_add(x_j, source, dim=0, dim_size=num_nodes) #adding neighbors features w.r.t source node + source node features\n",
        "    #out is (num_connected_node, num_features)\n",
        "\n",
        "    #update node embedding\n",
        "    out = self.cat([x, out], dim=1)   #nodes, num_feat*2\n",
        "    out = self.lin(out)     #node, out_channel\n",
        "    if self.normalize:\n",
        "      out = F.normalize(out)\n",
        "\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "FjcM6JcNDnR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#implementing the GAT layer\n",
        "class GAT(MessagePassing):\n",
        "  def __init__(self, in_channels, out_channels, heads=2, negative_slope=0.2,\n",
        "               dropout=0., **kwargs):\n",
        "    super().__init__(node_dim=0, **kwargs)   #changing node_dim value from parent class\n",
        "\n",
        "    self.in_channels = in_channels\n",
        "    self.out_channels = out_channels\n",
        "    self.heads = heads\n",
        "    self.negative_slope = negative_slope\n",
        "    self.dropout = dropout\n",
        "\n",
        "\n",
        "    self.lin_l = self.lin_r = nn.Linear(in_channels, out_channels * heads)\n",
        "    self.att_l = nn.Parameter(torch.Tensor(heads, out_channels))\n",
        "    self.att_r = nn.Parameter(torch.Tensor(heads, out_channels))  #one per node\n",
        "\n",
        "    self.reset_params()\n",
        "\n",
        "  def reset_params(self):\n",
        "    nn.init.xavier_uniform_(self.in_l.weight)\n",
        "    nn.init.xavier_uniform_(self.att_l)\n",
        "    nn.init.xavier_uniform_(self.att_r)\n",
        "\n",
        "  def forward(self, x, edge_index, size=None):\n",
        "\n",
        "    C, H = self.out_channels, self.heads\n",
        "\n",
        "    lt_x_l = self.lin_l(x).view(-1, H, C)\n",
        "    lt_x_r = self.lin_r(x).view(-1, H, C)\n",
        "\n",
        "    alpha_l = (lt_x_l * self.att_l).sum(dim=1, keepdim=False)\n",
        "    alpha_r = (lt_x_r * self.att_r).sum(dim=1, keepdim=False)\n",
        "    out = self.propagate(edge_index, x=(lt_x_l, lt_x_r), alpha=(alpha_l, alpha_r), size=size)\n",
        "    return out.view(-1, H * C)\n",
        "\n",
        "\n",
        "  def message(self, x_j, alpha_j, alpha_i, index, ptr, size_i):\n",
        "    alpha = F.leaky_relu(alpha_i + alpha_j, self.negative_slope)\n",
        "    alpha = softmax(alpha, index, ptr, size_i)\n",
        "    alpha = F.dropout(alpha, p=self.dropout, training=self.training)\n",
        "    return x_j * alpha.unsqueeze(1)\n",
        "\n",
        "\n",
        "  def aggregate(self, inputs, index, dim_size=None):\n",
        "    return scatter(inputs, index, dim=self.node_dim, dim_size=dim_size, reduce=\"sum\")     #node_dim is the axis along which to propagate message passing.(N, H, C)  node dim is N, hence 0\n"
      ],
      "metadata": {
        "id": "z4Bwezhz1VNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#implementing the GAT using pytorch\n",
        "class GATD(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, heads=2, negative_slope=0.2, dropout=0.):\n",
        "    super().__init__()\n",
        "    self.in_channels = in_channels\n",
        "    self.out_channels = out_channels\n",
        "    self.heads = heads\n",
        "    self.negative_slope = negative_slope\n",
        "    self.dropout = dropout\n",
        "\n",
        "    self.lin = nn.Linear(in_channels, out_channels * heads, bias=False)\n",
        "    self.att_l = nn.Parameter(torch.Tensor(1, heads, out_channels)) #NB torch.tensor takes input tensor not shape\n",
        "    self.att_r = nn.Parameter(torch.Tensor(1, heads, out_channels))\n",
        "    self.reset_parameters()\n",
        "\n",
        "  def reset_parameters(self):\n",
        "    nn.init.xavier_uniform_(self.lin.weight)\n",
        "    nn.init.xavier_uniform_(self.att_l)\n",
        "    nn.init.xavier_uniform_(self.att_r)\n",
        "\n",
        "  def forward(self, x, edge_index, size=None):\n",
        "    H, C = self.heads, self.out_channels\n",
        "\n",
        "    #accounting for sparseTensor\n",
        "    if isinstance(edge_index, SparseTensor):\n",
        "      source, target = edge_index.to_dense()\n",
        "    else:\n",
        "      source, target = edge_index\n",
        "\n",
        "    #Add self-loops to the adjacency list\n",
        "    num_nodes = x.size(0)\n",
        "    loop_index = torch.arange(0, num_nodes, dtype=torch.long, device=x.device)\n",
        "    loop_index = loop_index.unsqueeze(0).repeat(2, 1)\n",
        "    edge_index = torch.cat([edge_index, loop_index], dim=1)\n",
        "\n",
        "    #Linearly Transform feature matrix (Node, C*H)\n",
        "    x = self.lin(x).view(-1, H, C)\n",
        "\n",
        "    #Compute attention coefficients\n",
        "    alpha_l = (x * self.att_l).sum(dim=-1)    #for source node  NE, H (one sum for each head)\n",
        "    alpha_r = (x * self.att_r).sum(dim=-1)    #for target node\n",
        "    alpha = F.leaky_relu(alpha_l[source] + alpha_r[target], self.negative_slope) #sum alpha from source and target for each edge\n",
        "\n",
        "    #normalize w.r.t each source node\n",
        "    alpha = scatter_softmax(alpha, source, dim=0)  #NE, H  normalize each head separately w.r.t source hence dim=0\n",
        "\n",
        "    #add dropout\n",
        "    alpha = F.dropout(alpha, p=self.dropout, training=self.training)\n",
        "\n",
        "    #neighborhood aggregation\n",
        "    out = scatter_add(alpha.unsqueeze(-1) * x[target], source, dim=0, dim_size=num_nodes)\n",
        "\n",
        "    return out.view(-1, H * C)   #output is H*C which will serve as node embedding\n",
        "\n"
      ],
      "metadata": {
        "id": "TsbzQ6Uz1VS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "def build_optimizer(args, params):\n",
        "  weight_decay = args.weight_decay\n",
        "  fliter_fn = filter(lambda p: p.requires_grad, params)\n",
        "  if args.opt == \"adam\":\n",
        "    optimizer = optim.Adam(filter_fn, lr=args.lr, weight_decay=weight_decay)\n",
        "  elif args.opt == \"sgd\":\n",
        "    optimizer = optim.SGD(filter_fn, lr=args.lr, weight_decay=weight_decay)\n",
        "  elif args.opt == \"rmsprop\":\n",
        "    optimizer = optim.RMSprop(filter_fn, lr=args.lr, weight_decay=weight_decay)\n",
        "  elif args.opt == \"adagrad\":\n",
        "    optimizer = optim.Adagrad(filter_fn, lr=args.lr, weight_decay=weight_decay)\n",
        "  if not args.opt_scheduler:\n",
        "    return None, optimizer\n",
        "  elif args.opt_scheduler == \"step\":\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=args.opt_decay_step, gamma=args.opt_decay_rate)\n",
        "  elif args.opt_scheduler == \"cos\":\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.opt_restart)\n",
        "  return scheduler, optimizer"
      ],
      "metadata": {
        "id": "xgI9OjQH1VWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "from tqdm import trange\n",
        "import pandas as pd\n",
        "import copy\n",
        "\n",
        "from torch_geometric.datasets import TUDataset\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from torch_geometric.data import DataLoader\n",
        "import torch_geometric.nn as pyg_nn\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def train(dataset, args):\n",
        "  print(\"Node task test size:\", dataset[0]['test_mask'].sum().item())\n",
        "  test_loader = loader = DataLoader(dataset, batch_size=args.batch_size, shuffle=False)\n",
        "\n",
        "  #model\n",
        "  model = GNNStack(dataset.num_node_features, args.hidden_dim, dataset.num_classes, args)\n",
        "  sch, opt = build_optimizer(args, model.parameters())\n",
        "\n",
        "  # training\n",
        "  losses = []\n",
        "  best_acc = 0\n",
        "  test_accs = []\n",
        "  best_model = None\n",
        "  for epoch in trange(args.epochs, desc=\"Training\", unit=\"Epoch\"):\n",
        "    total_loss = 0\n",
        "    model.train()\n",
        "    for batch in loader:\n",
        "      opt.zero_grad()\n",
        "      pred = model(batch)\n",
        "      label = batch.y\n",
        "      pred = pred[batch.train_mask]    #semi-supervised. not all nodes habe label\n",
        "      label = label[batch.train_mask]\n",
        "      loss = model.loss(pred, label)\n",
        "      loss.backward()\n",
        "      opt.step()\n",
        "      total_loss += loss.item() * batch.num_graphs\n",
        "    total_loss /= len(loader.dataset)\n",
        "    losses.append(total_loss)\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "      test_acc = test(test_loader, model)\n",
        "      test_accs.append(test_acc)\n",
        "      if test_acc > best_acc:\n",
        "        best_acc = test_acc\n",
        "        best_model = copy.deepcopy(model)\n",
        "  return test_accs, losses, best_model, best_acc, test_loader\n",
        "\n",
        "\n",
        "\n",
        "def test(loader, test_model, is_validation=False, save_model_preds=False,\n",
        "         model_type=None):\n",
        "  test_model.eval()\n",
        "  correct = 0\n",
        "  for data in loader:\n",
        "    with torch.no_grad():\n",
        "      #pred = test_model(data).max(dim=1)[1]  max returns the indicies that contains the max\n",
        "      pred = test_model(data)\n",
        "      pred = pred.argmax(dim=1) # Batch, !\n",
        "\n",
        "    mask = data.val_mask if is_validation else data.test_mask\n",
        "\n",
        "    pred = pred[mask]\n",
        "    label = data.y[mask]\n",
        "\n",
        "    if save_model_preds:\n",
        "      print(\"Saving Model Predictions\", model_type)\n",
        "\n",
        "      data = {}\n",
        "      data['pred'] = pred.view(-1).cpu().detach().numpy()\n",
        "      data['label'] = label.view(-1).cpu().detach().numpy()\n",
        "\n",
        "      df = pd.DataFrame(data=data)\n",
        "      df.to_csv(f\"Node-{model_type}.csv\", sep=',', index=False)\n",
        "\n",
        "    correct += pred.eq(label).sum().item()   #eq is element-wise comparison. returns same shape of 1 if T and 0 if F\n",
        "\n",
        "  total = 0\n",
        "  for data in loader.dataset:\n",
        "    total += torch.sum(data.test_mask).item()\n",
        "  return correct / total\n",
        "\n",
        "\n",
        "\n",
        "class objectview(object):      #base class for python object\n",
        "  def __init__(self, d):\n",
        "    self.__dict__ = d\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "u6PM2TT2Rf-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for args in [\n",
        "        {'model_type': 'GAT', 'dataset': 'cora', 'num_layers': 2, 'heads': 1, 'batch_size': 32, 'hidden_dim': 32, 'dropout': 0.5, 'epochs': 500, 'opt': 'adam', 'opt_scheduler': 'none', 'opt_restart': 0, 'weight_decay': 5e-3, 'lr': 0.01},\n",
        "    ]:\n",
        "    args = objectview(args)\n",
        "\n",
        "    for model in [\"GAT\", \"GraphSage\", \"GATD\", \"GraphSageD\"]:\n",
        "      args.model_type = model\n",
        "      if model in [\"GAT\", \"GATD\"]:\n",
        "        args.heads = 2\n",
        "      else:\n",
        "        args.heads = 1\n",
        "\n",
        "      for dataset in [\"cora\", \"citeseer\", \"pubmed\"]:\n",
        "        args.dataset = dataset\n",
        "\n",
        "        if args.dataset == \"cora\":\n",
        "          dataset = Planetoid(root=\"/tmp/cora\", name=\"Cora\")\n",
        "        elif args.dataset == \"citeseer\":\n",
        "          dataset = Planetoid(root=\"/tmp/citeseer\", name=\"Citeseer\")\n",
        "        elif args.dataset == \"pubmed\":\n",
        "          dataset = Planetoid(root=\"/tmp/pubmed\", name=\"PubMed\")\n",
        "        else:\n",
        "          raise NotImplementedError(f\"Unknown dataset: {args.dataset}\")\n",
        "\n",
        "        test_accs, losses, best_model, best_acc, test_loader = train(dataset, args)\n",
        "\n",
        "         print(\"Maximum test set accuracy: {0}\".format(max(test_accs)))\n",
        "            print(\"Minimum loss: {0}\".format(min(losses)))\n",
        "\n",
        "            # Run test for our best model to save the predictions!\n",
        "            test(test_loader, best_model, is_validation=False, save_model_preds=True, model_type=model)\n",
        "            print()\n",
        "\n",
        "            plt.title(dataset.name)\n",
        "            plt.plot(losses, label=\"training loss\" + \" - \" + args.model_type)\n",
        "            plt.plot(test_accs, label=\"test accuracy\" + \" - \" + args.model_type)\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "y2t4dZKYRgBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3H0a9g7DRgG7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}